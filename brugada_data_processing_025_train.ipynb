{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wfdb import processing\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import data_visualisation as dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data\n",
    "brugada_path = 'DAT China/DAT China Brugada ECGs/dat'\n",
    "RBBB_path = 'DAT China/DAT China RBBB ECGs/datnew'\n",
    "Normal_path = 'DAT China/Normal/dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_china_data(dir_path, label):\n",
    "    \n",
    "    # init array\n",
    "    data = []\n",
    "    \n",
    "    # read all files in the directory\n",
    "    for file in os.listdir(dir_path):\n",
    "        path = os.path.join(dir_path, file)\n",
    "        f = open(path, 'r')\n",
    "        ecg = np.fromfile(f, dtype=np.int16)\n",
    "        ecg = np.reshape(ecg, (8, 5000))\n",
    "        \n",
    "        ## Downsample to 100hz\n",
    "        ecg = ecg.reshape(8, 1000, 5)\n",
    "        ecg = np.mean(ecg, axis=2)\n",
    "        \n",
    "        data.append(ecg)\n",
    "        f.close()\n",
    "        \n",
    "    # Convert to numpy array\n",
    "    data = np.array(data)\n",
    "    # Reshape to (n_samples, n_samples_per_lead, n_leads)\n",
    "    data = np.swapaxes(data, 1, 2)\n",
    "    \n",
    "    # Calculate remaining leads\n",
    "    lead_III = data[:,:,1] - data[:,:,0]           # ecg_dict['III'] = ecg_dict['II'] - ecg_dict['I']\n",
    "    lead_aVR = -0.5 * (data[:,:,0] + data[:,:,1])  # ecg_dict['aVR'] = -0.5 * (ecg_dict['I'] + ecg_dict['II'])\n",
    "    lead_aVL = data[:,:,0] - 0.5 * data[:,:,1]     # ecg_dict['aVL'] = ecg_dict['I'] - 0.5 * ecg_dict['II']\n",
    "    lead_aVF = data[:,:,1] - 0.5 * data[:,:,0]     # ecg_dict['aVF'] = ecg_dict['II'] - 0.5 * ecg_dict['I']\n",
    "    \n",
    "    # Add remaining leads to data\n",
    "    data = np.concatenate((data,  \n",
    "                              np.expand_dims(lead_III, axis=2), \n",
    "                              np.expand_dims(lead_aVR, axis=2), \n",
    "                              np.expand_dims(lead_aVL, axis=2), \n",
    "                              np.expand_dims(lead_aVF, axis=2)), axis=2)\n",
    "    \n",
    "    return data, np.full(data.shape[0], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: (9998, 1000, 12)\n",
      "Brugada: (176, 1000, 12)\n",
      "RBBB: (10000, 1000, 12)\n"
     ]
    }
   ],
   "source": [
    "Normal_data, normal_labels = load_china_data(Normal_path, 0)\n",
    "brugada_data, brugada_labels = load_china_data(brugada_path, 1)\n",
    "RBBB_data, RBBB_labels = load_china_data(RBBB_path, 2)\n",
    "\n",
    "print(\"Normal:\", Normal_data.shape)\n",
    "print(\"Brugada:\", brugada_data.shape)\n",
    "print(\"RBBB:\", RBBB_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: (352, 1000, 12)\n",
      "Brugada: (176, 1000, 12)\n",
      "RBBB: (176, 1000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Normalise data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_data = np.concatenate((Normal_data, brugada_data, RBBB_data), axis=0)\n",
    "scaler = StandardScaler()\n",
    "sclaer = scaler.fit(all_data.reshape(-1, all_data.shape[-1]))\n",
    "\n",
    "Normal_data = scaler.transform(Normal_data.reshape(-1, Normal_data.shape[-1])).reshape(Normal_data.shape)\n",
    "brugada_data = scaler.transform(brugada_data.reshape(-1, brugada_data.shape[-1])).reshape(brugada_data.shape)\n",
    "RBBB_data = scaler.transform(RBBB_data.reshape(-1, RBBB_data.shape[-1])).reshape(RBBB_data.shape)\n",
    "\n",
    "\n",
    "# Downsmaple classes\n",
    "RBBB_data = RBBB_data[np.random.choice(RBBB_data.shape[0], brugada_data.shape[0], replace=False)]\n",
    "Normal_data = Normal_data[np.random.choice(Normal_data.shape[0], 2*brugada_data.shape[0], replace=False)]\n",
    "\n",
    "print(\"Normal:\", Normal_data.shape)\n",
    "print(\"Brugada:\", brugada_data.shape)\n",
    "print(\"RBBB:\", RBBB_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape: (704, 1000, 12)\n",
      "Labels shape: (704,)\n",
      "Text labels shape: (704,)\n"
     ]
    }
   ],
   "source": [
    "# shuffle data\n",
    "np.random.shuffle(Normal_data)\n",
    "np.random.shuffle(RBBB_data)\n",
    "np.random.shuffle(brugada_data)\n",
    "\n",
    "data = np.concatenate((Normal_data, RBBB_data, brugada_data), axis=0)\n",
    "labels = np.concatenate((normal_labels[:Normal_data.shape[0]], RBBB_labels[:RBBB_data.shape[0]], brugada_labels), axis=0)\n",
    "\n",
    "fnc = lambda x: \"Normal\" if x == 0 else \"Brugada\" if x == 2 else \"RBBB\"\n",
    "text_labels = np.array([fnc(label) for label in labels])\n",
    "\n",
    "print(\"\\nData shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Text labels shape:\", text_labels.shape)\n",
    "\n",
    "# shuffle data\n",
    "permutation = np.random.permutation(data.shape[0])\n",
    "np.random.shuffle(permutation)\n",
    "\n",
    "shuffled_data = data[permutation]\n",
    "shuffled_labels = labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train label distribution:\n",
      "Total: 176\n",
      "Normal: 89 ( 0.5056818181818182 %)\n",
      "Brugada: 42 ( 0.23863636363636365 %)\n",
      "RBBB: 45 ( 0.2556818181818182 %)\n",
      "\n",
      "Test label distribution:\n",
      "Total: 528\n",
      "Normal: 263 ( 0.4981060606060606 %)\n",
      "Brugada: 134 ( 0.2537878787878788 %)\n",
      "RBBB: 131 ( 0.2481060606060606 %)\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.75\n",
    "\n",
    "test_split = int(data.shape[0] * test_split)\n",
    "\n",
    "X_train = shuffled_data[test_split:]\n",
    "Y_train = shuffled_labels[test_split:]\n",
    "Y_train_text = text_labels[test_split:]\n",
    "\n",
    "X_test = shuffled_data[:test_split]\n",
    "Y_test = shuffled_labels[:test_split]\n",
    "Y_test_text = text_labels[:test_split]\n",
    "\n",
    "# label distribution\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(\"Total:\", Y_train.shape[0])\n",
    "print(\"Normal:\", np.sum(Y_train == 0), \"(\", np.sum(Y_train == 0) / Y_train.shape[0], \"%)\")\n",
    "print(\"Brugada:\", np.sum(Y_train == 1), \"(\", np.sum(Y_train == 1) / Y_train.shape[0], \"%)\")\n",
    "print(\"RBBB:\", np.sum(Y_train == 2), \"(\", np.sum(Y_train == 2) / Y_train.shape[0], \"%)\")\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(\"Total:\", Y_test.shape[0])\n",
    "print(\"Normal:\", np.sum(Y_test == 0), \"(\", np.sum(Y_test == 0) / Y_test.shape[0], \"%)\")\n",
    "print(\"Brugada:\", np.sum(Y_test == 1), \"(\", np.sum(Y_test == 1) / Y_test.shape[0], \"%)\")\n",
    "print(\"RBBB:\", np.sum(Y_test == 2), \"(\", np.sum(Y_test == 2) / Y_test.shape[0], \"%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_DATA_LR = 'data/DAT_China_025.npz'\n",
    "\n",
    "save_args = {\n",
    "    'X_train': X_train,\n",
    "    'Y_train': Y_train,\n",
    "    'Y_train_text': Y_train_text,\n",
    "    'X_test': X_test,\n",
    "    'Y_test': Y_test,\n",
    "    'Y_test_text': Y_test_text\n",
    "}\n",
    "\n",
    "np.savez(NP_DATA_LR, **save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data:\n",
      "X_train: (176, 1000, 12)\n",
      "Y_train: (176,)\n",
      "Y_train_text: (176,)\n",
      "\n",
      "X_test: (528, 1000, 12)\n",
      "Y_test: (528,)\n",
      "Y_test_text: (528,)\n"
     ]
    }
   ],
   "source": [
    "# Load data and check\n",
    "\n",
    "thismodule = sys.modules[__name__]\n",
    "NP_DATA = 'data/DAT_China_025.npz'\n",
    "\n",
    "with np.load(NP_DATA, allow_pickle=True) as data:\n",
    "    for k in data.keys():\n",
    "        if 'text' in k:\n",
    "            setattr(thismodule, k, data[k])\n",
    "        else:\n",
    "            setattr(thismodule, k, data[k].astype(float))\n",
    "            \n",
    "print(\"\\nLoaded data:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"Y_train_text:\", Y_train_text.shape)\n",
    "\n",
    "print(\"\\nX_test:\", X_test.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n",
    "print(\"Y_test_text:\", Y_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data into folds for cross validation\n",
    "k = 5\n",
    "train_len = X_train.shape[0]\n",
    "fold_size = train_len // k\n",
    "\n",
    "# create a list mapping each sample to a fold\n",
    "folds = np.zeros(train_len, dtype=int)\n",
    "for i in range(k):\n",
    "    folds[i*fold_size:(i+1)*fold_size] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_beats(X, Y, Y_text, folds=None, window_size=50):\n",
    "    X_HB = []\n",
    "    Y_HB = []\n",
    "    Y_HB_text = []\n",
    "    Folds_HB = []\n",
    "    \n",
    "    get_text_label = lambda x: label_dict[x]\n",
    "    \n",
    "    for i, sample in enumerate(X):\n",
    "        print(\"Progress:\", i, \"/\", X.shape[0], end=\"\\r\")\n",
    "        xqrs = processing.XQRS(sig=sample[:,0], fs=sampling_rate)\n",
    "        xqrs.detect(verbose=False)\n",
    "        qrs_inds = xqrs.qrs_inds\n",
    "        for indx in qrs_inds:\n",
    "            if indx + window_size > sample.shape[0] or indx - window_size < 0:\n",
    "                continue\n",
    "            X_HB.append(sample[indx-window_size:indx+window_size])\n",
    "            Y_HB.append(Y[i])\n",
    "            Y_HB_text.append(Y_text[i])\n",
    "            if folds is not None:\n",
    "                Folds_HB.append(folds[i])\n",
    "    \n",
    "    return np.array(X_HB), np.array(Y_HB), np.array(Y_HB_text), np.array(Folds_HB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Extracted 2624 heartbeats from 176 samples\n",
      "Test set: Extracted 7887 heartbeats from 528 samples\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 100\n",
    "window_size = int(sampling_rate * 0.5)\n",
    "\n",
    "X_train_HB, Y_train_HB, Y_train_HB_text, folds_train_HB = split_into_beats(X_train, Y_train, Y_train_text, folds=folds, window_size=window_size)\n",
    "X_test_HB, Y_test_HB, Y_test_HB_text, _ = split_into_beats(X_test, Y_test, Y_test_text, window_size=window_size)\n",
    "\n",
    "print(f\"Train set: Extracted {X_train_HB.shape[0]} heartbeats from {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: Extracted {X_test_HB.shape[0]} heartbeats from {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RBBB labels to normal\n",
    "Y_train_HB[Y_train_HB == 2] = 0\n",
    "Y_test_HB[Y_test_HB == 2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_DATA_HB = 'data/DAT_China_1s_025.npz'\n",
    "\n",
    "save_args = {\n",
    "    'X_train': X_train_HB,\n",
    "    'Y_train': Y_train_HB,\n",
    "    'Y_train_text': Y_train_HB_text,\n",
    "    'folds_train': folds_train_HB,\n",
    "    'X_test': X_test_HB,\n",
    "    'Y_test': Y_test_HB,\n",
    "    'Y_test_text': Y_test_HB_text\n",
    "}\n",
    "\n",
    "np.savez(NP_DATA_HB, **save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "X_train: (2624, 100, 12)\n",
      "Y_train: (2624,)\n",
      "Y_train_text: (2624,)\n",
      "Folds_train: (2624,)\n",
      "\n",
      "Test set:\n",
      "X_test: (7887, 100, 12)\n",
      "Y_test: (7887,)\n",
      "Y_test_text: (7887,)\n",
      "(array([0., 1., 2., 3., 4.]), array([480, 539, 544, 529, 532]))\n"
     ]
    }
   ],
   "source": [
    "# load and check\n",
    "thismodule = sys.modules[__name__]\n",
    "NP_DATA = 'data/DAT_China_1s_025.npz'\n",
    "\n",
    "with np.load(NP_DATA, allow_pickle=True) as data:\n",
    "    for k in data.keys():\n",
    "        if 'text' in k:\n",
    "            setattr(thismodule, k, data[k])\n",
    "        else:\n",
    "            setattr(thismodule, k, data[k].astype(float))\n",
    "            \n",
    "print(\"Train set:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"Y_train_text:\", Y_train_text.shape)\n",
    "print(\"Folds_train:\", folds_train.shape)\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n",
    "print(\"Y_test_text:\", Y_test_text.shape)\n",
    "\n",
    "print(np.unique(folds_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 size: 480\n",
      "Fold 0 true labels: 148.0\n",
      "Fold 1 size: 539\n",
      "Fold 1 true labels: 144.0\n",
      "Fold 2 size: 544\n",
      "Fold 2 true labels: 74.0\n",
      "Fold 3 size: 529\n",
      "Fold 3 true labels: 110.0\n",
      "Fold 4 size: 532\n",
      "Fold 4 true labels: 120.0\n",
      "Fold 0 label counts: (array([0., 1.]), array([332, 148]))\n",
      "Fold 1 label counts: (array([0., 1.]), array([395, 144]))\n",
      "Fold 2 label counts: (array([0., 1.]), array([470,  74]))\n",
      "Fold 3 label counts: (array([0., 1.]), array([419, 110]))\n",
      "Fold 4 label counts: (array([0., 1.]), array([412, 120]))\n"
     ]
    }
   ],
   "source": [
    "# label dist of each fold  \n",
    "# print counts of each label in each fold\n",
    "for i in range(5):\n",
    "    print(f'Fold {i} label counts: {np.unique(Y_train[folds_train == i], return_counts=True)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels to binary\n",
    "Y_train = np.where(Y_train == 1, True, False)\n",
    "Y_test = np.where(Y_test == 1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 shape:           (480, 100, 12)\n",
      "Resampled Fold 0 shape: (664, 100, 12)\n",
      "Fold 0 class distribution:           [332 148]\n",
      "Resampled Fold 0 class distribution: [332 332]\n",
      "\n",
      "Fold 1 shape:           (539, 100, 12)\n",
      "Resampled Fold 1 shape: (790, 100, 12)\n",
      "Fold 1 class distribution:           [395 144]\n",
      "Resampled Fold 1 class distribution: [395 395]\n",
      "\n",
      "Fold 2 shape:           (544, 100, 12)\n",
      "Resampled Fold 2 shape: (940, 100, 12)\n",
      "Fold 2 class distribution:           [470  74]\n",
      "Resampled Fold 2 class distribution: [470 470]\n",
      "\n",
      "Fold 3 shape:           (529, 100, 12)\n",
      "Resampled Fold 3 shape: (838, 100, 12)\n",
      "Fold 3 class distribution:           [419 110]\n",
      "Resampled Fold 3 class distribution: [419 419]\n",
      "\n",
      "Fold 4 shape:           (532, 100, 12)\n",
      "Resampled Fold 4 shape: (824, 100, 12)\n",
      "Fold 4 class distribution:           [412 120]\n",
      "Resampled Fold 4 class distribution: [412 412]\n",
      "(2624, 100, 12) (2624,) (2624,)\n",
      "(4056, 100, 12) (4056,) (4056,)\n",
      "(1432, 100, 12)\n"
     ]
    }
   ],
   "source": [
    "# smote per fold\n",
    "\n",
    "# empty arrays to store the resampled data of shape (0, 100, 12)\n",
    "Resampled_X_train = np.array([], dtype=np.float32).reshape(0, 100, 12)\n",
    "Resampled_Y_train = np.array([], dtype=bool).reshape(0,)\n",
    "Resampled_folds_train = np.array([], dtype=int).reshape(0,)\n",
    "\n",
    "new_X_train = np.array([], dtype=np.float32).reshape(0, 100, 12)\n",
    "\n",
    "for k in range(5):\n",
    "    # get the fold samples + labels\n",
    "    X_fold = X_train[folds_train == k]\n",
    "    Y_fold = Y_train[folds_train == k]\n",
    "    X_fold_flattened = X_fold.reshape((X_fold.shape[0], -1))\n",
    "    \n",
    "    # resample the fold\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled_fold_flattened, y_resampled_fold = smote.fit_resample(X_fold_flattened, Y_fold)\n",
    "    X_resampled_fold = X_resampled_fold_flattened.reshape((-1, 100, 12))\n",
    "    \n",
    "    new_X = X_resampled_fold[X_fold.shape[0]:]\n",
    "    \n",
    "    folds = np.full((X_resampled_fold.shape[0],), k)\n",
    "\n",
    "    print(f\"\\nFold {k} shape:           {X_fold.shape}\")\n",
    "    print(f\"Resampled Fold {k} shape: {X_resampled_fold.shape}\")\n",
    "    print(f\"Fold {k} class distribution:           {np.bincount(Y_fold)}\")\n",
    "    print(f\"Resampled Fold {k} class distribution: {np.bincount(y_resampled_fold)}\")\n",
    "\n",
    "    \n",
    "    # concatenate the resampled fold with the original data\n",
    "    Resampled_X_train = np.concatenate([Resampled_X_train, X_resampled_fold])\n",
    "    Resampled_Y_train = np.concatenate([Resampled_Y_train, y_resampled_fold])\n",
    "    Resampled_folds_train = np.concatenate([Resampled_folds_train, folds])\n",
    "    \n",
    "    new_X_train = np.concatenate([new_X_train, new_X])\n",
    "    \n",
    "    \n",
    "print(X_train.shape, Y_train.shape, folds_train.shape)\n",
    "print(Resampled_X_train.shape, Resampled_Y_train.shape, Resampled_folds_train.shape)\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the resampled data per fold\n",
    "np.savez_compressed('data/DAT_China_1s_025_SMOTE.npz',\n",
    "                    X_train=X_train, Y_train=Y_train,\n",
    "                    X_train_resampled=Resampled_X_train, Y_train_resampled=Resampled_Y_train,\n",
    "                    folds_train=folds_train, folds_train_resampled=Resampled_folds_train,\n",
    "                    X_test=X_test, Y_test=Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
